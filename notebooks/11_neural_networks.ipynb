{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-12T20:10:11.163989Z",
     "start_time": "2024-12-12T20:10:09.872342Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:10:12.050411Z",
     "start_time": "2024-12-12T20:10:11.170222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load data\n",
    "data = pd.read_parquet('../cache/encoded_99q_scaled.parquet')"
   ],
   "id": "f03b740df0f7c595",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:10:12.192716Z",
     "start_time": "2024-12-12T20:10:12.189300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GDELT(Dataset):\n",
    "    def __init__(self, features, target):\n",
    "        if hasattr(features, 'values'):\n",
    "            features = features.values\n",
    "        if hasattr(target, 'values'):\n",
    "            target = target.values\n",
    "           \n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.target = torch.tensor(target, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.target[idx]"
   ],
   "id": "293439ff67b6a91a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:10:12.220048Z",
     "start_time": "2024-12-12T20:10:12.213181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PricePredictor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # simple neural network with 3 layers (linear)\n",
    "        self.network = nn.Sequential(\n",
    "            # 1st layer\n",
    "            nn.Linear(input_dim, 128), # input layer to hidden layer 1 with 128 neurons\n",
    "            nn.BatchNorm1d(128), # batch normalization layer to normalize the output of the 1st layer before activation function is applied to it\n",
    "            nn.ReLU(), # activation function to introduce non-linearity to the model output from the 1st layer\n",
    "            nn.Dropout(0.3), # dropout layer to prevent overfitting by randomly setting 30% of the output from the 1st layer to 0\n",
    "        \n",
    "            # 2nd layer\n",
    "            nn.Linear(128, 64), # hidden layer 1 to hidden layer 2 with 64 neurons\n",
    "            nn.BatchNorm1d(64), # batch normalization layer\n",
    "            nn.ReLU(), # activation function\n",
    "            nn.Dropout(0.3), # dropout layer\n",
    "        \n",
    "            # 3rd layer\n",
    "            nn.Linear(64, 32), # hidden layer 2 to hidden layer 3 with 32 neurons\n",
    "            nn.BatchNorm1d(32), # batch normalization layer\n",
    "            nn.ReLU(), # activation function\n",
    "            nn.Dropout(0.3), # dropout layer\n",
    "        \n",
    "            # output layer\n",
    "            nn.Linear(32, 1) # hidden layer 3 to output layer with 1 neuron\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ],
   "id": "a79894d2e5735342",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:10:12.257385Z",
     "start_time": "2024-12-12T20:10:12.248342Z"
    }
   },
   "cell_type": "code",
   "source": "data.columns",
   "id": "514519bd36ec2724",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Timestamp', 'Actor1Country', 'Actor1GeoCountry', 'Actor1Type',\n",
       "       'Actor2Country', 'Actor2GeoCountry', 'Actor2Type', 'ActionCountry',\n",
       "       'EventType', 'GoldsteinScale', 'NumSources', 'NumArticles', 'AvgTone',\n",
       "       'Magnitude', 'Impact', 'Impact_bin', 'pct_change_15min',\n",
       "       'pct_change_30min', 'pct_change_24h', 'AbsChange'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:10:12.306905Z",
     "start_time": "2024-12-12T20:10:12.304151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prep_data(df, target='pct_change_30min'):\n",
    "    feature_cols = [\n",
    "        'Actor1Country', 'Actor1GeoCountry', 'Actor1Type',\n",
    "        'Actor2Country', 'Actor2GeoCountry', 'Actor2Type',\n",
    "        'ActionCountry', 'EventType', 'GoldsteinScale',\n",
    "        'NumSources', 'NumArticles', 'AvgTone',\n",
    "        'Magnitude', 'Impact'\n",
    "    ]\n",
    "        \n",
    "    x = df[feature_cols]\n",
    "    y = df[target]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(x)\n",
    "    \n",
    "    return x_scaled, y, scaler"
   ],
   "id": "40984dcb08acc1bf",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:10:12.326735Z",
     "start_time": "2024-12-12T20:10:12.321792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(x, y, batch_size=256, epochs=50, learning_rate=0.001, patience=10):\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    train_idx, test_idx = list(tscv.split(x))[-1] # use the last split as test set\n",
    "    \n",
    "    x_train, y_train = x[train_idx], y[train_idx]\n",
    "    x_val, y_val = x[test_idx], y[test_idx]\n",
    "    \n",
    "    train_dataset = GDELT(x_train, y_train)\n",
    "    val_dataset = GDELT(x_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = PricePredictor(x.shape[1])\n",
    "    \n",
    "    criterion = nn.MSELoss() # loss function (mean squared error)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    best_model = None\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for features, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(features)\n",
    "            loss = criterion(output, target.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # val\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, target in val_loader:\n",
    "                output = model(features)\n",
    "                loss = criterion(output, target.reshape(-1, 1))\n",
    "                val_loss += loss.item()\n",
    "                predictions.extend(output.numpy().flatten())\n",
    "                actuals.extend(target.numpy().flatten())\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, train loss: {train_loss:.4f}, val loss: {val_loss:.4f}, accuracy: {np.mean(np.sign(predictions) == np.sign(actuals))}')\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter == patience:\n",
    "                print(f'Early stopping at epoch {epoch+1} with val loss: {val_loss:.4f}')\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(best_model.state_dict())\n",
    "    return model"
   ],
   "id": "efb9f0b31df5409f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:10:12.348614Z",
     "start_time": "2024-12-12T20:10:12.337008Z"
    }
   },
   "cell_type": "code",
   "source": "data.head()",
   "id": "40c9b1e978879593",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            Timestamp  Actor1Country  Actor1GeoCountry  Actor1Type  \\\n",
       "Date                                                                 \n",
       "2019-01-01  -1.530752              9                13           9   \n",
       "2019-01-01  -1.530752              9                13           9   \n",
       "2019-01-01  -1.530752              9                13           9   \n",
       "2019-01-01  -1.530752              9                13           9   \n",
       "2019-01-01  -1.530752              9                13           9   \n",
       "\n",
       "            Actor2Country  Actor2GeoCountry  Actor2Type  ActionCountry  \\\n",
       "Date                                                                     \n",
       "2019-01-01             13                11           9             11   \n",
       "2019-01-01             13                11           9             11   \n",
       "2019-01-01             13                11           9             11   \n",
       "2019-01-01             13                11           9             11   \n",
       "2019-01-01             13                11           9             11   \n",
       "\n",
       "            EventType  GoldsteinScale  NumSources  NumArticles   AvgTone  \\\n",
       "Date                                                                       \n",
       "2019-01-01         10        0.605751    0.889228     2.004333  1.164239   \n",
       "2019-01-01         10        0.731167    0.367070     0.597142  0.468568   \n",
       "2019-01-01         10        0.563945    4.022178     1.300737  1.004049   \n",
       "2019-01-01         10       -0.523000    0.367070    -0.036094 -0.520616   \n",
       "2019-01-01         10        0.292209    1.933545    -0.247173  0.921836   \n",
       "\n",
       "            Magnitude    Impact         Impact_bin  pct_change_15min  \\\n",
       "Date                                                                   \n",
       "2019-01-01   0.375252  0.714246           Positive         -0.033061   \n",
       "2019-01-01  -0.716820  0.425568  Slightly Positive         -0.033061   \n",
       "2019-01-01   0.960180  0.848379           Positive         -0.033061   \n",
       "2019-01-01   0.334805 -0.355905  Slightly Negative         -0.033061   \n",
       "2019-01-01  -0.100779  0.329342  Slightly Positive         -0.033061   \n",
       "\n",
       "            pct_change_30min  pct_change_24h  AbsChange  \n",
       "Date                                                     \n",
       "2019-01-01         -0.226363       -2.433464  -0.199873  \n",
       "2019-01-01         -0.226363       -2.433464  -0.199873  \n",
       "2019-01-01         -0.226363       -2.433464  -0.199873  \n",
       "2019-01-01         -0.226363       -2.433464  -0.199873  \n",
       "2019-01-01         -0.226363       -2.433464  -0.199873  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Actor1Country</th>\n",
       "      <th>Actor1GeoCountry</th>\n",
       "      <th>Actor1Type</th>\n",
       "      <th>Actor2Country</th>\n",
       "      <th>Actor2GeoCountry</th>\n",
       "      <th>Actor2Type</th>\n",
       "      <th>ActionCountry</th>\n",
       "      <th>EventType</th>\n",
       "      <th>GoldsteinScale</th>\n",
       "      <th>NumSources</th>\n",
       "      <th>NumArticles</th>\n",
       "      <th>AvgTone</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Impact</th>\n",
       "      <th>Impact_bin</th>\n",
       "      <th>pct_change_15min</th>\n",
       "      <th>pct_change_30min</th>\n",
       "      <th>pct_change_24h</th>\n",
       "      <th>AbsChange</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>-1.530752</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0.605751</td>\n",
       "      <td>0.889228</td>\n",
       "      <td>2.004333</td>\n",
       "      <td>1.164239</td>\n",
       "      <td>0.375252</td>\n",
       "      <td>0.714246</td>\n",
       "      <td>Positive</td>\n",
       "      <td>-0.033061</td>\n",
       "      <td>-0.226363</td>\n",
       "      <td>-2.433464</td>\n",
       "      <td>-0.199873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>-1.530752</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0.731167</td>\n",
       "      <td>0.367070</td>\n",
       "      <td>0.597142</td>\n",
       "      <td>0.468568</td>\n",
       "      <td>-0.716820</td>\n",
       "      <td>0.425568</td>\n",
       "      <td>Slightly Positive</td>\n",
       "      <td>-0.033061</td>\n",
       "      <td>-0.226363</td>\n",
       "      <td>-2.433464</td>\n",
       "      <td>-0.199873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>-1.530752</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0.563945</td>\n",
       "      <td>4.022178</td>\n",
       "      <td>1.300737</td>\n",
       "      <td>1.004049</td>\n",
       "      <td>0.960180</td>\n",
       "      <td>0.848379</td>\n",
       "      <td>Positive</td>\n",
       "      <td>-0.033061</td>\n",
       "      <td>-0.226363</td>\n",
       "      <td>-2.433464</td>\n",
       "      <td>-0.199873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>-1.530752</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.523000</td>\n",
       "      <td>0.367070</td>\n",
       "      <td>-0.036094</td>\n",
       "      <td>-0.520616</td>\n",
       "      <td>0.334805</td>\n",
       "      <td>-0.355905</td>\n",
       "      <td>Slightly Negative</td>\n",
       "      <td>-0.033061</td>\n",
       "      <td>-0.226363</td>\n",
       "      <td>-2.433464</td>\n",
       "      <td>-0.199873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>-1.530752</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0.292209</td>\n",
       "      <td>1.933545</td>\n",
       "      <td>-0.247173</td>\n",
       "      <td>0.921836</td>\n",
       "      <td>-0.100779</td>\n",
       "      <td>0.329342</td>\n",
       "      <td>Slightly Positive</td>\n",
       "      <td>-0.033061</td>\n",
       "      <td>-0.226363</td>\n",
       "      <td>-2.433464</td>\n",
       "      <td>-0.199873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:10:13.907942Z",
     "start_time": "2024-12-12T20:10:12.400736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = data.sample(1000000)\n",
    "sample.reset_index(drop=True, inplace=True)"
   ],
   "id": "335f8891bc9707ad",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:15:21.984073Z",
     "start_time": "2024-12-12T20:10:13.982931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features),\n",
    "            nn.LayerNorm(in_features),\n",
    "            nn.GELU(),  # GELU instead of ReLU/LeakyReLU\n",
    "            nn.Linear(in_features, in_features),\n",
    "            nn.LayerNorm(in_features),\n",
    "        )\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gelu(x + self.block(x))\n",
    "\n",
    "class PricePredictor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial feature extraction\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            ResidualBlock(32),\n",
    "            ResidualBlock(32),\n",
    "            ResidualBlock(32)\n",
    "        )\n",
    "        \n",
    "        # Price movement prediction\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LayerNorm(16),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight, gain=1e-2)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extraction(x)\n",
    "        x = self.residual_blocks(x)\n",
    "        return self.predictor(x)\n",
    "\n",
    "def train_model(X, y, batch_size=128, epochs=100):\n",
    "    \"\"\"Train with improved monitoring\"\"\"\n",
    "    \n",
    "    # Split data - use last 20% for validation\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    if hasattr(X_train, 'values'):\n",
    "        X_train = X_train.values\n",
    "        X_val = X_val.values\n",
    "    if hasattr(y_train, 'values'):\n",
    "        y_train = y_train.values\n",
    "        y_val = y_val.values\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_train),\n",
    "        torch.FloatTensor(y_train)\n",
    "    )\n",
    "    val_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_val),\n",
    "        torch.FloatTensor(y_val)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = PricePredictor(input_dim=X.shape[1])\n",
    "    \n",
    "    # Custom loss combining MSE and directional accuracy\n",
    "    class DirectionalMSELoss(nn.Module):\n",
    "        def __init__(self, alpha=0.7):\n",
    "            super().__init__()\n",
    "            self.alpha = alpha\n",
    "            self.mse = nn.MSELoss()\n",
    "\n",
    "        def forward(self, pred, target):\n",
    "            mse_loss = self.mse(pred, target)\n",
    "            # Directional loss\n",
    "            direction_loss = torch.mean(\n",
    "                1 - torch.sign(pred) * torch.sign(target)\n",
    "            )\n",
    "            return self.alpha * mse_loss + (1 - self.alpha) * direction_loss\n",
    "\n",
    "    criterion = DirectionalMSELoss()\n",
    "    \n",
    "    # Optimizer with cosine annealing\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=0.001,\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=10,  # Reset every 10 epochs\n",
    "        T_mult=2  # Double the reset interval after each reset\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_directions = []\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Calculate direction accuracy\n",
    "            pred_direction = torch.sign(outputs.detach())\n",
    "            true_direction = torch.sign(batch_y.reshape(-1, 1))\n",
    "            train_directions.append(\n",
    "                (pred_direction == true_direction).float().mean().item()\n",
    "            )\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_directions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                val_loss = criterion(outputs, batch_y.reshape(-1, 1))\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                pred_direction = torch.sign(outputs)\n",
    "                true_direction = torch.sign(batch_y.reshape(-1, 1))\n",
    "                val_directions.append(\n",
    "                    (pred_direction == true_direction).float().mean().item()\n",
    "                )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        train_dir_acc = np.mean(train_directions)\n",
    "        val_dir_acc = np.mean(val_directions)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}]')\n",
    "        print(f'Train Loss: {train_loss:.6f}, Dir Acc: {train_dir_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.6f}, Dir Acc: {val_dir_acc:.4f}')\n",
    "        print(f'LR: {scheduler.get_last_lr()[0]:.6f}\\n')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "\n",
    "X, y, scaler = prep_data(sample)\n",
    "\n",
    "# Train model\n",
    "model = train_model(X, y)"
   ],
   "id": "ff66e408a0b1e65c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "Train Loss: 0.520032, Dir Acc: 0.5016\n",
      "Val Loss: 0.522415, Dir Acc: 0.4926\n",
      "LR: 0.000976\n",
      "\n",
      "Epoch [2/100]\n",
      "Train Loss: 0.520240, Dir Acc: 0.5012\n",
      "Val Loss: 0.522415, Dir Acc: 0.4926\n",
      "LR: 0.000905\n",
      "\n",
      "Epoch [3/100]\n",
      "Train Loss: 0.519965, Dir Acc: 0.5017\n",
      "Val Loss: 0.515539, Dir Acc: 0.5041\n",
      "LR: 0.000794\n",
      "\n",
      "Epoch [4/100]\n",
      "Train Loss: 0.519951, Dir Acc: 0.5017\n",
      "Val Loss: 0.515519, Dir Acc: 0.5041\n",
      "LR: 0.000655\n",
      "\n",
      "Epoch [5/100]\n",
      "Train Loss: 0.519857, Dir Acc: 0.5019\n",
      "Val Loss: 0.515518, Dir Acc: 0.5041\n",
      "LR: 0.000500\n",
      "\n",
      "Epoch [6/100]\n",
      "Train Loss: 0.519464, Dir Acc: 0.5025\n",
      "Val Loss: 0.515519, Dir Acc: 0.5041\n",
      "LR: 0.000345\n",
      "\n",
      "Epoch [7/100]\n",
      "Train Loss: 0.518988, Dir Acc: 0.5033\n",
      "Val Loss: 0.515517, Dir Acc: 0.5041\n",
      "LR: 0.000206\n",
      "\n",
      "Epoch [8/100]\n",
      "Train Loss: 0.518632, Dir Acc: 0.5039\n",
      "Val Loss: 0.515518, Dir Acc: 0.5041\n",
      "LR: 0.000095\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 204\u001B[0m\n\u001B[1;32m    201\u001B[0m X, y, scaler \u001B[38;5;241m=\u001B[39m prep_data(sample)\n\u001B[1;32m    203\u001B[0m \u001B[38;5;66;03m# Train model\u001B[39;00m\n\u001B[0;32m--> 204\u001B[0m model \u001B[38;5;241m=\u001B[39m train_model(X, y)\n",
      "Cell \u001B[0;32mIn[10], line 145\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(X, y, batch_size, epochs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;66;03m# Gradient clipping\u001B[39;00m\n\u001B[1;32m    143\u001B[0m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), max_norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m)\n\u001B[0;32m--> 145\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m    146\u001B[0m train_losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[1;32m    148\u001B[0m \u001B[38;5;66;03m# Calculate direction accuracy\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:137\u001B[0m, in \u001B[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    135\u001B[0m opt \u001B[38;5;241m=\u001B[39m opt_ref()\n\u001B[1;32m    136\u001B[0m opt\u001B[38;5;241m.\u001B[39m_opt_called \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n\u001B[0;32m--> 137\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__get__\u001B[39m(opt, opt\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    482\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    483\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    484\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    485\u001B[0m             )\n\u001B[0;32m--> 487\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    488\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    490\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     90\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 91\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/optim/adamw.py:220\u001B[0m, in \u001B[0;36mAdamW.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    207\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m cast(Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m], group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    209\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    210\u001B[0m         group,\n\u001B[1;32m    211\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    217\u001B[0m         state_steps,\n\u001B[1;32m    218\u001B[0m     )\n\u001B[0;32m--> 220\u001B[0m     adamw(\n\u001B[1;32m    221\u001B[0m         params_with_grad,\n\u001B[1;32m    222\u001B[0m         grads,\n\u001B[1;32m    223\u001B[0m         exp_avgs,\n\u001B[1;32m    224\u001B[0m         exp_avg_sqs,\n\u001B[1;32m    225\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    226\u001B[0m         state_steps,\n\u001B[1;32m    227\u001B[0m         amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[1;32m    228\u001B[0m         beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    229\u001B[0m         beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    230\u001B[0m         lr\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    231\u001B[0m         weight_decay\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    232\u001B[0m         eps\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    233\u001B[0m         maximize\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    234\u001B[0m         foreach\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mforeach\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    235\u001B[0m         capturable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcapturable\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    236\u001B[0m         differentiable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    237\u001B[0m         fused\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfused\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    238\u001B[0m         grad_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad_scale\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    239\u001B[0m         found_inf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    240\u001B[0m         has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    241\u001B[0m     )\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001B[0m, in \u001B[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/optim/adamw.py:782\u001B[0m, in \u001B[0;36madamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    779\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    780\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adamw\n\u001B[0;32m--> 782\u001B[0m func(\n\u001B[1;32m    783\u001B[0m     params,\n\u001B[1;32m    784\u001B[0m     grads,\n\u001B[1;32m    785\u001B[0m     exp_avgs,\n\u001B[1;32m    786\u001B[0m     exp_avg_sqs,\n\u001B[1;32m    787\u001B[0m     max_exp_avg_sqs,\n\u001B[1;32m    788\u001B[0m     state_steps,\n\u001B[1;32m    789\u001B[0m     amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[1;32m    790\u001B[0m     beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    791\u001B[0m     beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    792\u001B[0m     lr\u001B[38;5;241m=\u001B[39mlr,\n\u001B[1;32m    793\u001B[0m     weight_decay\u001B[38;5;241m=\u001B[39mweight_decay,\n\u001B[1;32m    794\u001B[0m     eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[1;32m    795\u001B[0m     maximize\u001B[38;5;241m=\u001B[39mmaximize,\n\u001B[1;32m    796\u001B[0m     capturable\u001B[38;5;241m=\u001B[39mcapturable,\n\u001B[1;32m    797\u001B[0m     differentiable\u001B[38;5;241m=\u001B[39mdifferentiable,\n\u001B[1;32m    798\u001B[0m     grad_scale\u001B[38;5;241m=\u001B[39mgrad_scale,\n\u001B[1;32m    799\u001B[0m     found_inf\u001B[38;5;241m=\u001B[39mfound_inf,\n\u001B[1;32m    800\u001B[0m     has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    801\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/optim/adamw.py:372\u001B[0m, in \u001B[0;36m_single_tensor_adamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001B[0m\n\u001B[1;32m    369\u001B[0m step_t \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;66;03m# Perform stepweight decay\u001B[39;00m\n\u001B[0;32m--> 372\u001B[0m param\u001B[38;5;241m.\u001B[39mmul_(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m lr \u001B[38;5;241m*\u001B[39m weight_decay)\n\u001B[1;32m    374\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[1;32m    375\u001B[0m exp_avg\u001B[38;5;241m.\u001B[39mlerp_(grad, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:15:22.104504Z",
     "start_time": "2024-12-08T20:47:08.524639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save model\n",
    "import joblib\n",
    "joblib.dump(model, '../cache/nn_model.joblib')"
   ],
   "id": "8b074dc6833fe4e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../cache/nn_model.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
